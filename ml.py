import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
from google.colab import drive
from sklearn.model_selection import cross_val_score
import seaborn as sns
import pickle
from sklearn.externals import joblib
from sklearn.naive_bayes import GaussianNB
import seaborn as sns
from sklearn.naive_bayes import BernoulliNB
from sklearn import tree
from sklearn.linear_model import LogisticRegression


df = pd.read_csv ('/content/BTP.csv')

print (df.shape)

drive.mount('/content/drive')

df.drop (columns = ['Unnamed: 0', 'Unnamed: 530'], inplace = True)
x = df.sum (axis = 0)
type (x)
keys = x.keys()
values = x.values
count = 0
to_rem_1 = []
for i in range (530):
    if (values[i] == 1 or values[i] > 600):
        count += 1
        to_rem_1.append (keys[i])
print (count)

df2 = df.drop (columns = to_rem_1)

df2 = df2.sample (frac = 1) # reshuffle
df2.head(10)

df_col_list = list (df2)
df_col_list.remove ("Result")

# New feature
df2["Sum"] = df2[df_col_list].sum (axis = 1)

sns.set_theme(style="darkgrid")
sns.set(rc={'figure.figsize':(15, 15)})




ax = sns.countplot(x="Sum", data=df2, hue = "Result")

x = df2.drop (columns=['Result'])
y = df2['Result']

clf = RandomForestClassifier(random_state=7)


x = x.values

clf.fit(x,y)
scores = cross_val_score(clf, x, y, cv = 10)
scores.mean()

 

# saving as file
joblib.dump(clf, 'clf_rf.pkl')


y_pred = cross_val_predict(clf, x, y, cv=10)
conf_mat = confusion_matrix(y, y_pred)





sns.heatmap(conf_mat, annot=True)

sns.heatmap(conf_mat/np.sum(conf_mat), annot=True, fmt='.2%', cmap='Blues')

# saving as file
joblib.dump(clf, 'clf_rf.pkl')

# loading model 
model_name1 = joblib.load('clf_rf.pkl')

# using model

x = df2.drop (columns=['Result'])
y = df2['Result']


clf_gnb = GaussianNB()
x = x.values

clf_gnb.fit(x,y)

scores = cross_val_score(clf_gnb, x, y, cv = 10)
scores.mean()



# saving as file
joblib.dump(clf_gnb, 'clf_gnb.pkl')


y_pred = cross_val_predict(clf_gnb, x, y, cv=10)
conf_mat = confusion_matrix(y, y_pred)

sns.heatmap(conf_mat, annot=True)
sns.heatmap(conf_mat/np.sum(conf_mat), annot=True, fmt='.2%', cmap='Blues')

# decision tree
x = df2.drop (columns=['Result'])
y = df2['Result']

clf_dt = tree.DecisionTreeClassifier()
x = x.values

clf_dt.fit(x,y)
scores = cross_val_score(clf_dt, x, y, cv = 10)
scores.mean()


# saving as file
joblib.dump(clf_dt, 'clf_dt.pkl')


y_pred = cross_val_predict(clf_dt, x, y, cv=10)
conf_mat = confusion_matrix(y, y_pred)
sns.heatmap(conf_mat/np.sum(conf_mat), annot=True, fmt='.2%', cmap='Blues')


# Bernoulli naive bayes

clf_bnb = BernoulliNB()
x = x.values

clf_bnb.fit(x,y)
scores = cross_val_score(clf_bnb, x, y, cv = 10)
scores.mean()


# saving as file
joblib.dump(clf_bnb, 'clf_bnb.pkl')

y_pred = cross_val_predict(clf_bnb, x, y, cv=10)
conf_mat = confusion_matrix(y, y_pred)
sns.heatmap(conf_mat/np.sum(conf_mat), annot=True, fmt='.2%', cmap='Blues')



# Logisitc

clf_log = LogisticRegression(max_iter = 1000)

x = x.values

clf_log.fit(x,y)
scores = cross_val_score(clf_log, x, y, cv = 10)
scores.mean()


y_pred = cross_val_predict(clf_log, x, y, cv=10)
conf_mat = confusion_matrix(y, y_pred)
sns.heatmap(conf_mat/np.sum(conf_mat), annot=True, fmt='.2%', cmap='Blues')
#saving 
joblib.dump(clf_log, 'clf_log.pkl')